{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into training and test data \n",
    "The data comes in two different files, one for the training set and one for the testing set. I then seperated it into the X and Y variables. Finally I converted them to numpy arrays in order to use them later to fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('fashion-mnist_test.csv')\n",
    "train = pd.read_csv('fashion-mnist_train.csv')\n",
    "xtest=test.drop([\"label\"], axis=1)\n",
    "ytest=test[\"label\"]\n",
    "xtrain=train.drop([\"label\"], axis=1)\n",
    "ytrain=train[\"label\"]\n",
    "x_train = np.asarray(xtrain)\n",
    "y_train = np.asarray(ytrain)\n",
    "x_test = np.asarray(xtest)\n",
    "y_test = np.asarray(ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function setup\n",
    "This function is to create the desired model and will be called in future with various parameter values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_layer, num_neuron):\n",
    "    model = tf.keras.Sequential()\n",
    "    #here we are using the truncated normal distribution to set the initial weights\n",
    "    kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=2 / np.sqrt(len(x_train) + num_neuron))\n",
    "    model.add(tf.keras.layers.Dense(num_neuron, input_shape=(28 * 28,), activation='relu', kernel_initializer=kernel_initializer))\n",
    "    for i in range(num_layer):\n",
    "        model.add(tf.keras.layers.Dense(num_neuron, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search\n",
    "using random search to find the hyperparameter values with the highest accuracy. I used the entire training set for this because the random search function uses cross validation so it is not necessary to pull a validation set from the training set when choosing the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
       "                   estimator=<tensorflow.python.keras.wrappers.scikit_learn.KerasClassifier object at 0x1a2fe8c320>,\n",
       "                   iid='warn', n_iter=10, n_jobs=None,\n",
       "                   param_distributions={'num_layer': [1, 2, 3, 4],\n",
       "                                        'num_neuron': [96, 112, 128, 144]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_layer = [1, 2, 3, 4]\n",
    "num_neuron = [96, 112, 128, 144]\n",
    "# Must create a dictionary that contains the hyperparameters and their values for the param_distributions\n",
    "param_dist = dict(num_layer=num_layer, num_neuron=num_neuron)\n",
    "# Creating the estimator to pass into the random search function\n",
    "model = tf.keras.wrappers.scikit_learn.KerasClassifier(build_fn=create_model, epochs=40, batch_size=500, verbose=0)\n",
    "random_search = RandomizedSearchCV(estimator=model,param_distributions=param_dist, n_iter=10, cv=3)\n",
    "random_search.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random search results\n",
    "here the results show the highest accuracy was obtained with number of neurons being set to 112 \n",
    "and number of layers being set to 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.877117 using {'num_neuron': 112, 'num_layer': 4}\n",
      "0.864883 (0.004818) with: {'num_neuron': 128, 'num_layer': 3}\n",
      "0.872567 (0.001636) with: {'num_neuron': 144, 'num_layer': 2}\n",
      "0.865050 (0.003900) with: {'num_neuron': 96, 'num_layer': 2}\n",
      "0.872417 (0.007068) with: {'num_neuron': 144, 'num_layer': 4}\n",
      "0.863817 (0.001438) with: {'num_neuron': 96, 'num_layer': 1}\n",
      "0.877117 (0.002388) with: {'num_neuron': 112, 'num_layer': 4}\n",
      "0.867267 (0.002243) with: {'num_neuron': 96, 'num_layer': 3}\n",
      "0.865717 (0.001838) with: {'num_neuron': 112, 'num_layer': 2}\n",
      "0.865567 (0.005147) with: {'num_neuron': 144, 'num_layer': 1}\n",
      "0.864033 (0.004308) with: {'num_neuron': 128, 'num_layer': 1}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best: %f using %s\" % (random_search.best_score_, random_search.best_params_))\n",
    "means = random_search.cv_results_['mean_test_score']\n",
    "stds = random_search.cv_results_['std_test_score']\n",
    "params = random_search.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Final Model\n",
    "now we fit the model with the optimized hyperparameters to the training set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "60000/60000 [==============================] - 7s 117us/sample - loss: 0.6577 - accuracy: 0.7856\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a34136eb8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model=create_model(random_search.best_params_[\"num_layer\"], random_search.best_params_[\"num_neuron\"])\n",
    "final_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model Results\n",
    "the results below show that the accuracy on the test set is approximately 81%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8126\n"
     ]
    }
   ],
   "source": [
    "results = final_model.evaluate(x_test,y_test, verbose=0)\n",
    "print(\"Accuracy:\", results[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
